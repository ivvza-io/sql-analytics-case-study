{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c004475",
   "metadata": {},
   "source": [
    "# Analytical Consumption Demo (Semantic Layer)\n",
    "\n",
    "This notebook demonstrates how the SQL semantic layer enables reproducible analytical consumption with minimal data wrangling.\n",
    "\n",
    "**Scope**\n",
    "- Load a small set of semantic views\n",
    "- Build one analysis-ready dataset at the heat level\n",
    "- Show two simple examples (segmentation + quick visualization)\n",
    "\n",
    "Deep analysis (SPC / root-cause / statistical modeling) is intentionally out of scope and covered in separate case studies.\n",
    "\n",
    "\n",
    "> **Confidentiality note**  \n",
    "> This notebook demonstrates real analytical consumption patterns (semantic views + joins + light reshaping).  \n",
    "> The original production database and data are not included in this public repository due to confidentiality constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect a DATABASE_URL like:\n",
    "# postgresql+psycopg2://user:password@host:5432/dbname\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "if not DATABASE_URL:\n",
    "    raise ValueError(\n",
    "        \"Missing DATABASE_URL env var. \"\n",
    "        \"Example: export DATABASE_URL='postgresql+psycopg2://user:pass@localhost:5432/aluminum_model_dev'\"\n",
    "    )\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Semantic Views\n",
    "\n",
    "We load only a small, focused set of semantic views for this demo.\n",
    "- `v_heats_by_alloy`\n",
    "- `v_heats_by_final_product`\n",
    "- `v_lab_values_by_heats` (tensile, analysis sessions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_heats_alloy = \"\"\"\n",
    "select\n",
    "  heat_num,\n",
    "  alloy_code\n",
    "from v_heats_by_alloy\n",
    "\"\"\"\n",
    "\n",
    "q_final_prod  = \"\"\"\n",
    "select\n",
    "  heat_num,\n",
    "  base_temper,\n",
    "  strain_level,\n",
    "  product_form,\n",
    "  thickness,\n",
    "  width\n",
    "from v_heats_by_final_product\n",
    "\"\"\"\n",
    "\n",
    "# Lab-first: tensile results, analysis sessions, selected metrics\n",
    "q_lab = \"\"\"\n",
    "select\n",
    "  heat_num,\n",
    "  lab_test,\n",
    "  test_value,\n",
    "  session_type,\n",
    "  test_type\n",
    "from v_lab_values_by_heats\n",
    "where test_type = 'tensile'\n",
    "  and session_type = 'analysis'\n",
    "  and lab_test in ('UTS','YS','Elongation')\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    heats_alloy = pd.read_sql(text(q_heats_alloy), conn)\n",
    "    final_prod  = pd.read_sql(text(q_final_prod),  conn)\n",
    "    lab         = pd.read_sql(text(q_lab),         conn)\n",
    "\n",
    "heats_alloy.head(), final_prod.head(), lab.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Analysis-Ready Dataset\n",
    "\n",
    "We pivot lab metrics to keep **1 row per heat** and then join semantic views using `heat_num`.\n",
    "\n",
    "This replaces multi-file spreadsheet merges and manual wrangling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot lab metrics so we keep 1 row per heat\n",
    "lab_wide = (\n",
    "    lab.pivot_table(\n",
    "        index=\"heat_num\",\n",
    "        columns=\"lab_test\",\n",
    "        values=\"test_value\",\n",
    "        aggfunc=\"mean\"\n",
    "    )\n",
    "    .rename(columns={\n",
    "        \"UTS\": \"uts_value\",\n",
    "        \"YS\": \"ys_value\",\n",
    "        \"Elongation\": \"elongation_value\",\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df = (\n",
    "    heats_alloy\n",
    "    .merge(final_prod, on=\"heat_num\", how=\"inner\")\n",
    "    .merge(lab_wide, on=\"heat_num\", how=\"inner\")\n",
    ")\n",
    "\n",
    "df = df[[\n",
    "    \"heat_num\",\n",
    "    \"alloy_code\",\n",
    "    \"base_temper\",\n",
    "    \"strain_level\",\n",
    "    \"product_form\",\n",
    "    \"thickness\",\n",
    "    \"width\",\n",
    "    \"uts_value\",\n",
    "    \"ys_value\",\n",
    "    \"elongation_value\",\n",
    "]]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Analysis #1 — Simple Segmentation\n",
    "\n",
    "Example segmentation:\n",
    "- `product_form = 'disc'`\n",
    "- `base_temper = 'O'`\n",
    "- `thickness` between `0.80` and `1.20`\n",
    "\n",
    "This produces a small summary table that can be reused for reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = df[\n",
    "    (df[\"product_form\"] == \"circles\") &\n",
    "    (df[\"base_temper\"] == \"O\") &\n",
    "    (df[\"thickness\"].between(0.80, 1.20))\n",
    "].copy()\n",
    "\n",
    "summary = (\n",
    "    segment\n",
    "    .groupby([\"alloy_code\", \"product_form\", \"base_temper\"], as_index=False)\n",
    "    .agg(\n",
    "        n=(\"heat_num\", \"size\"),\n",
    "        avg_uts=(\"uts_value\", \"mean\"),\n",
    "        sd_uts=(\"uts_value\", \"std\"),\n",
    "        avg_ys=(\"ys_value\", \"mean\"),\n",
    "        avg_elong=(\"elongation_value\", \"mean\"),\n",
    "    )\n",
    "    .sort_values([\"n\", \"avg_uts\"], ascending=[False, False])\n",
    ")\n",
    "\n",
    "summary.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Analysis #2 — Quick Visualization\n",
    "\n",
    "A single simple chart to validate that the dataset is ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = segment.dropna(subset=[\"uts_value\"]) \n",
    "\n",
    "plt.figure()\n",
    "segment[\"uts_value\"].hist(bins=30)\n",
    "plt.title(\"UTS distribution — O-temper discs (0.80–1.20 mm)\")\n",
    "plt.xlabel(\"UTS\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Notes\n",
    "\n",
    "- This workflow consumes **semantic views** directly, avoiding raw spreadsheet wrangling.\n",
    "- The analysis dataset is built through simple joins using `heat_num` as a consistent key.\n",
    "- The same notebook can be rerun for new data without changing transformation logic.\n",
    "- Deeper analyses (SPC, control charts, root-cause studies) are intentionally covered in separate case studies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
